{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read me\n",
    "\n",
    "Unsupervised autoencoder learning by using all the images (train + test)\n",
    "\n",
    "This autoencoder is constructed based on residual network\n",
    "\n",
    "Input images: 128 x 128\n",
    "\n",
    "Output = Input images\n",
    "\n",
    "embedding length: 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from math import floor, ceil, pi\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load paths of all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total image number: 25460\n",
      "['./data/whale/train_full/00022e1a.jpg', './data/whale/train_full/000466c4.jpg', './data/whale/train_full/00087b01.jpg', './data/whale/train_full/001296d5.jpg', './data/whale/train_full/0014cfdf.jpg', './data/whale/train_full/0025e8c2.jpg', './data/whale/train_full/0026a8ab.jpg', './data/whale/train_full/0031c258.jpg', './data/whale/train_full/0035632e.jpg', './data/whale/train_full/0037e7d3.jpg', './data/whale/train_full/00389cd7.jpg', './data/whale/train_full/0042dcc4.jpg', './data/whale/train_full/0042ea34.jpg', './data/whale/train_full/00467ae9.jpg', './data/whale/train_full/004a97f3.jpg', './data/whale/train_full/004c5fb9.jpg', './data/whale/train_full/005c57e7.jpg', './data/whale/train_full/006d0aaf.jpg', './data/whale/train_full/0078af23.jpg', './data/whale/train_full/007c3603.jpg']\n"
     ]
    }
   ],
   "source": [
    "def get_image_paths(folder):\n",
    "#     folder = './data/whale/train_full'\n",
    "    files = os.listdir(folder)\n",
    "    files.sort()\n",
    "    files = ['{}/{}'.format(folder, file) for file in files]\n",
    "    return files\n",
    "\n",
    "X_img_paths_train = get_image_paths('./data/whale/train_full')\n",
    "X_img_paths_test = get_image_paths('./data/whale/test')\n",
    "X_img_paths = X_img_paths_train + X_img_paths_test\n",
    "print \"total image number: %d\" % len(X_img_paths)\n",
    "print(X_img_paths[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation: to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_augmentation(original_image):\n",
    "    #to do\n",
    "    \n",
    "    return original_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to gray scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image resize and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25460, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 128\n",
    "\n",
    "def tf_resize_augment_images(X_img_file_paths):\n",
    "    X_data = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, (None, None, 1))\n",
    "    tf_img = tf.image.resize_images(X, (IMAGE_SIZE, IMAGE_SIZE), tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Each image is resized individually as different image may be of different size.\n",
    "        for index, file_path in enumerate(X_img_file_paths):\n",
    "            img = mpimg.imread(file_path)\n",
    "            if len(img.shape) > 2:# convert to grayscale\n",
    "                img = rgb2gray(img)\n",
    "            img = img.reshape(img.shape[0], img.shape[1], 1)\n",
    "            resized_img = sess.run(tf_img, feed_dict = {X: img})\n",
    "            X_data.append(data_augmentation(resized_img))\n",
    "\n",
    "    X_data = np.array(X_data, dtype = np.float32) # Convert to numpy\n",
    "    return X_data\n",
    "\n",
    "X_imgs = tf_resize_augment_images(X_img_paths)\n",
    "print(X_imgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save processed images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./data/whale/save/resize_all.npy', X_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_imgs = np.load('./data/whale/save/resize_all.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, subtract, merge, Dense, Flatten\n",
    "from keras.layers import MaxPooling2D, BatchNormalization, LeakyReLU, Activation, add, Conv2DTranspose\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "\n",
    "K.clear_session()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "image_train = X_imgs[:int((1 - test_ratio) * X_imgs.shape[0])]\n",
    "image_test = X_imgs[int((1 - test_ratio) * X_imgs.shape[0]):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normalization (subtract mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_normalize(image_train, image_test):\n",
    "    mean = image_train.mean(axis=0, keepdims=1)\n",
    "    image_train -= mean\n",
    "    image_test -= mean\n",
    "    return image_train, image_test\n",
    "\n",
    "image_train, image_test = image_normalize(image_train, image_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define residual network block for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resNet_block_encoder(image, channel, strides):\n",
    "    net = Conv2D(channel, kernel_size=(3, 3), strides=(strides, strides), kernel_initializer=\"he_normal\", padding='same')(image)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = Activation('relu')(net)\n",
    "    net = Conv2D(channel, kernel_size=(3, 3), strides=(1, 1), kernel_initializer=\"he_normal\", padding='same')(net)\n",
    "    \n",
    "    if strides > 1:\n",
    "        image = Conv2D(channel, kernel_size=(3, 3), strides=(strides, strides), kernel_initializer=\"he_normal\", padding='same')(image)\n",
    "        \n",
    "    net = add([image, net])\n",
    "    net = BatchNormalization()(net)\n",
    "    net = Activation('relu')(net)\n",
    "    \n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define residual network block for decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resNet_block_decoder(image, channel, strides, up=1):\n",
    "    net = Conv2DTranspose(channel, kernel_size=(3, 3), strides=(strides, strides), kernel_initializer=\"he_normal\", padding='same')(image)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = Activation('relu')(net)\n",
    "    net = Conv2DTranspose(channel, kernel_size=(3, 3), strides=(1, 1), kernel_initializer=\"he_normal\", padding='same')(net)\n",
    "    \n",
    "    if strides > 1 or up > 1:\n",
    "        image = Conv2DTranspose(channel, kernel_size=(3, 3), strides=(strides, strides), kernel_initializer=\"he_normal\", padding='same')(image)\n",
    "        \n",
    "    net = add([image, net])\n",
    "    net = BatchNormalization()(net)\n",
    "    net = Activation('relu')(net)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input shape: 128 x 128 x 1   output shape: 4 x 4 x 32\n",
    "def encoding(image):\n",
    "    net = Conv2D(128, kernel_size=(7, 7), strides=(2, 2), kernel_initializer=\"he_normal\", padding='same')(image)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = Activation('relu')(net)  \n",
    "    net = MaxPooling2D()(net)\n",
    "    \n",
    "    net = resNet_block_encoder(net, 128, 1)\n",
    "    net = resNet_block_encoder(net, 128, 1)\n",
    "    net = resNet_block_encoder(net, 128, 1)\n",
    "    \n",
    "    net = resNet_block_encoder(net, 64, 2)\n",
    "    net = resNet_block_encoder(net, 64, 1)\n",
    "    net = resNet_block_encoder(net, 64, 1)\n",
    "    \n",
    "    net = resNet_block_encoder(net, 32, 2)\n",
    "    net = resNet_block_encoder(net, 32, 1)\n",
    "    net = resNet_block_encoder(net, 32, 1)\n",
    "    \n",
    "    embedding = MaxPooling2D()(net)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding(embedding):    \n",
    "    net = resNet_block_decoder(embedding, 32, 1)\n",
    "    net = resNet_block_decoder(net, 32, 1)\n",
    "    net = resNet_block_decoder(net, 32, 2)\n",
    "    \n",
    "    net = resNet_block_decoder(net, 64, 1, up=64/32)\n",
    "    net = resNet_block_decoder(net, 64, 1)\n",
    "    net = resNet_block_decoder(net, 64, 2)\n",
    "    \n",
    "    net = resNet_block_decoder(net, 128, 1, up=128/64)\n",
    "    net = resNet_block_decoder(net, 128, 1)\n",
    "    net = resNet_block_decoder(net, 128, 1)\n",
    "    net = UpSampling2D()(net)\n",
    "    \n",
    "    net = Conv2DTranspose(128, kernel_size=(7, 7), strides=(2, 2), kernel_initializer=\"he_normal\", padding='same')(net)\n",
    "    net = BatchNormalization()(net)\n",
    "    net = Activation('relu')(net)  \n",
    "    net = UpSampling2D()(net)\n",
    "    \n",
    "    net = Conv2DTranspose(1, kernel_size=(5, 5), strides=(1, 1), kernel_initializer=\"he_normal\", padding='same')(net)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training with 1 GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"co..., inputs=Tensor(\"in...)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00001\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "inputs = Input(input_shape)\n",
    "G = 1 #the number of GPU\n",
    "\n",
    "def autoencoder(image):\n",
    "    encoded = encoding(image)\n",
    "    decoded = decoding(encoded)\n",
    "    if G <= 1:\n",
    "        print(\"[INFO] training with 1 GPU...\")\n",
    "        autoencoder_net = Model(input=image, output=decoded)\n",
    "    # otherwise, we are compiling using multiple GPUs\n",
    "    else:\n",
    "        print(\"[INFO] training with {} GPUs...\".format(G))\n",
    "\n",
    "        # we'll store a copy of the model on *every* GPU and then combine\n",
    "        # the results from the gradient updates on the CPU\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            # initialize the model\n",
    "            autoencoder_net = Model(input=image, output=decoded)\n",
    "\n",
    "        # make the model parallel\n",
    "        autoencoder_net = multi_gpu_model(autoencoder_net, gpus=G)    \n",
    "    \n",
    "    optimizer = Adam(learning_rate)\n",
    "    autoencoder_net.compile(loss=\"mean_squared_error\", optimizer=optimizer)\n",
    "    return autoencoder_net\n",
    "\n",
    "autoencoder_net = autoencoder(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\tloss_train:2405.7246094\tloss_test:2695.7152473\n",
      "epoch:1\tloss_train:2388.4062500\tloss_test:2270.2618942\n",
      "epoch:2\tloss_train:1748.1499023\tloss_test:2042.1111452\n",
      "epoch:3\tloss_train:1784.1038818\tloss_test:1894.9394279\n",
      "epoch:4\tloss_train:1926.6647949\tloss_test:1816.1314510\n",
      "epoch:5\tloss_train:1814.8316650\tloss_test:1721.0979462\n",
      "epoch:6\tloss_train:1565.5454102\tloss_test:1660.2603937\n",
      "epoch:7\tloss_train:1376.7740479\tloss_test:1610.7684163\n",
      "epoch:8\tloss_train:1609.4069824\tloss_test:1560.9160361\n",
      "epoch:9\tloss_train:1453.6972656\tloss_test:1519.3290659\n",
      "epoch:10\tloss_train:1492.0303955\tloss_test:1487.7703094\n",
      "epoch:11\tloss_train:1304.2666016\tloss_test:1448.1948216\n",
      "epoch:12\tloss_train:1583.6809082\tloss_test:1425.2619197\n",
      "epoch:13\tloss_train:1349.9971924\tloss_test:1395.2845746\n",
      "epoch:14\tloss_train:1686.7138672\tloss_test:1373.3602706\n",
      "epoch:15\tloss_train:1550.3935547\tloss_test:1348.0502297\n",
      "epoch:16\tloss_train:1575.4797363\tloss_test:1328.6479921\n",
      "epoch:17\tloss_train:1375.7734375\tloss_test:1311.5178743\n",
      "epoch:18\tloss_train:1259.6573486\tloss_test:1294.5326058\n",
      "epoch:19\tloss_train:1503.9313965\tloss_test:1287.6654483\n",
      "epoch:20\tloss_train:1273.0676270\tloss_test:1258.6551375\n",
      "epoch:21\tloss_train:1199.3746338\tloss_test:1246.8055526\n",
      "epoch:22\tloss_train:1279.4243164\tloss_test:1235.1157220\n",
      "epoch:23\tloss_train:1311.0814209\tloss_test:1220.9440861\n",
      "epoch:24\tloss_train:1041.3767090\tloss_test:1210.6473297\n",
      "epoch:25\tloss_train:1242.3853760\tloss_test:1204.0989961\n",
      "epoch:26\tloss_train:1246.3005371\tloss_test:1190.5025795\n",
      "epoch:27\tloss_train:1321.5107422\tloss_test:1188.6672346\n",
      "epoch:28\tloss_train:975.6405640\tloss_test:1180.7401609\n",
      "epoch:29\tloss_train:1093.0306396\tloss_test:1156.8870534\n",
      "epoch:30\tloss_train:1057.4080811\tloss_test:1151.3086629\n",
      "epoch:31\tloss_train:1195.2775879\tloss_test:1150.3878569\n",
      "epoch:32\tloss_train:1138.7572021\tloss_test:1132.3243819\n",
      "epoch:33\tloss_train:1158.9877930\tloss_test:1136.5105095\n",
      "epoch:34\tloss_train:1209.7872314\tloss_test:1119.3836922\n",
      "epoch:35\tloss_train:924.8115845\tloss_test:1112.2711615\n",
      "epoch:36\tloss_train:889.6064453\tloss_test:1105.9075142\n",
      "epoch:37\tloss_train:1060.6475830\tloss_test:1102.9056884\n",
      "epoch:38\tloss_train:932.1547852\tloss_test:1094.0544767\n",
      "epoch:39\tloss_train:1324.8891602\tloss_test:1090.3319637\n",
      "epoch:40\tloss_train:948.9442139\tloss_test:1089.2086031\n",
      "epoch:41\tloss_train:930.2652588\tloss_test:1078.9228729\n",
      "epoch:42\tloss_train:1107.8273926\tloss_test:1084.1286536\n",
      "epoch:43\tloss_train:904.7719727\tloss_test:1071.5560168\n",
      "epoch:44\tloss_train:933.1844482\tloss_test:1065.9167263\n",
      "epoch:45\tloss_train:1069.0115967\tloss_test:1058.2745977\n",
      "epoch:46\tloss_train:1148.3361816\tloss_test:1053.0307347\n",
      "epoch:47\tloss_train:1036.9420166\tloss_test:1050.4742747\n",
      "epoch:48\tloss_train:928.2075806\tloss_test:1045.3675086\n",
      "epoch:49\tloss_train:1001.2059937\tloss_test:1040.4590089\n",
      "epoch:50\tloss_train:1017.9567261\tloss_test:1038.0740699\n",
      "epoch:51\tloss_train:936.6146240\tloss_test:1033.8333964\n",
      "epoch:52\tloss_train:1078.0355225\tloss_test:1029.4918791\n",
      "epoch:53\tloss_train:956.4472656\tloss_test:1029.3169239\n",
      "epoch:54\tloss_train:918.2805786\tloss_test:1021.8797061\n",
      "epoch:55\tloss_train:1127.5864258\tloss_test:1025.3172547\n",
      "epoch:56\tloss_train:1186.0837402\tloss_test:1014.1735040\n",
      "epoch:57\tloss_train:817.7174072\tloss_test:1012.6081679\n",
      "epoch:58\tloss_train:926.8240356\tloss_test:1010.9599763\n",
      "epoch:59\tloss_train:964.3470459\tloss_test:1016.4196547\n",
      "epoch:60\tloss_train:882.4345093\tloss_test:1004.0788832\n",
      "epoch:61\tloss_train:845.4416504\tloss_test:1001.4087655\n",
      "epoch:62\tloss_train:973.5510254\tloss_test:997.6859708\n",
      "epoch:63\tloss_train:1018.7987061\tloss_test:995.1579861\n",
      "epoch:64\tloss_train:853.7009277\tloss_test:1005.7847375\n",
      "epoch:65\tloss_train:947.8400269\tloss_test:987.1597371\n",
      "epoch:66\tloss_train:952.9503174\tloss_test:983.8132577\n",
      "epoch:67\tloss_train:886.4740601\tloss_test:981.9008324\n",
      "epoch:68\tloss_train:1064.9360352\tloss_test:978.2439521\n",
      "epoch:69\tloss_train:928.6829224\tloss_test:978.9940323\n",
      "epoch:70\tloss_train:891.4750366\tloss_test:995.1688483\n",
      "epoch:71\tloss_train:916.0180664\tloss_test:985.7615795\n",
      "epoch:72\tloss_train:1023.1663818\tloss_test:969.0975162\n",
      "epoch:73\tloss_train:1004.3510742\tloss_test:969.8820836\n",
      "epoch:74\tloss_train:935.8945312\tloss_test:968.5977663\n",
      "epoch:75\tloss_train:1063.1247559\tloss_test:965.9821147\n",
      "epoch:76\tloss_train:871.6797485\tloss_test:967.0152911\n",
      "epoch:77\tloss_train:950.4038086\tloss_test:971.1251262\n",
      "epoch:78\tloss_train:846.9228516\tloss_test:955.9649662\n",
      "epoch:79\tloss_train:816.0629272\tloss_test:955.2910703\n",
      "epoch:80\tloss_train:812.2022095\tloss_test:953.1627257\n",
      "epoch:81\tloss_train:810.1801758\tloss_test:951.9393570\n",
      "epoch:82\tloss_train:808.5883179\tloss_test:949.2838090\n",
      "epoch:83\tloss_train:842.9327393\tloss_test:945.9345928\n",
      "epoch:84\tloss_train:1042.4417725\tloss_test:951.3354195\n",
      "epoch:85\tloss_train:801.4517212\tloss_test:943.8368074\n",
      "epoch:86\tloss_train:967.7060547\tloss_test:945.1418428\n",
      "epoch:87\tloss_train:846.8305664\tloss_test:940.0762928\n",
      "epoch:88\tloss_train:1050.0953369\tloss_test:937.1400255\n",
      "epoch:89\tloss_train:893.1900635\tloss_test:940.3022807\n",
      "epoch:90\tloss_train:936.8779297\tloss_test:935.2145897\n",
      "epoch:91\tloss_train:954.6872559\tloss_test:937.4602860\n",
      "epoch:92\tloss_train:829.3597412\tloss_test:933.8875621\n",
      "epoch:93\tloss_train:851.2380371\tloss_test:931.6413929\n",
      "epoch:94\tloss_train:761.1583252\tloss_test:933.4890759\n",
      "epoch:95\tloss_train:935.9993286\tloss_test:930.0195331\n",
      "epoch:96\tloss_train:889.5771484\tloss_test:923.7409025\n",
      "epoch:97\tloss_train:874.5322266\tloss_test:925.0520632\n",
      "epoch:98\tloss_train:941.5711060\tloss_test:926.5875844\n",
      "epoch:99\tloss_train:846.6171875\tloss_test:919.9539140\n",
      "epoch:100\tloss_train:769.5924072\tloss_test:919.4136705\n",
      "epoch:101\tloss_train:702.0664062\tloss_test:916.1172199\n",
      "epoch:102\tloss_train:850.6798706\tloss_test:914.8190771\n",
      "epoch:103\tloss_train:667.5148315\tloss_test:917.4095973\n",
      "epoch:104\tloss_train:796.0989990\tloss_test:912.8667092\n",
      "epoch:105\tloss_train:942.2262573\tloss_test:913.0663209\n",
      "epoch:106\tloss_train:856.4564209\tloss_test:913.8146457\n",
      "epoch:107\tloss_train:794.0899048\tloss_test:915.5573967\n",
      "epoch:108\tloss_train:839.8214722\tloss_test:909.5034314\n",
      "epoch:109\tloss_train:879.0059814\tloss_test:907.1282363\n",
      "epoch:110\tloss_train:834.5831909\tloss_test:906.9142354\n",
      "epoch:111\tloss_train:837.1962891\tloss_test:906.5439468\n",
      "epoch:112\tloss_train:821.4636230\tloss_test:909.6963038\n",
      "epoch:113\tloss_train:761.1412354\tloss_test:902.6386391\n",
      "epoch:114\tloss_train:905.8942261\tloss_test:900.5736075\n",
      "epoch:115\tloss_train:858.9006958\tloss_test:900.6252042\n",
      "epoch:116\tloss_train:871.8339233\tloss_test:904.5123516\n",
      "epoch:117\tloss_train:965.7401733\tloss_test:895.5235706\n",
      "epoch:118\tloss_train:739.5459595\tloss_test:909.7253400\n",
      "epoch:119\tloss_train:895.1932373\tloss_test:899.4062614\n",
      "epoch:120\tloss_train:909.5588379\tloss_test:891.9743981\n",
      "epoch:121\tloss_train:847.9558105\tloss_test:893.9120581\n",
      "epoch:122\tloss_train:814.8986206\tloss_test:896.8187105\n",
      "epoch:123\tloss_train:837.5919189\tloss_test:900.3817701\n",
      "epoch:124\tloss_train:939.1955566\tloss_test:895.2061263\n",
      "epoch:125\tloss_train:757.5730591\tloss_test:890.6191156\n",
      "epoch:126\tloss_train:813.4343262\tloss_test:894.0063816\n",
      "epoch:127\tloss_train:947.2264404\tloss_test:898.9430674\n",
      "epoch:128\tloss_train:816.6416626\tloss_test:888.0701858\n",
      "epoch:129\tloss_train:840.8115845\tloss_test:884.9290949\n",
      "epoch:130\tloss_train:701.6634521\tloss_test:884.1782970\n",
      "epoch:131\tloss_train:909.3672485\tloss_test:886.9631208\n",
      "epoch:132\tloss_train:815.2143555\tloss_test:897.4783377\n",
      "epoch:133\tloss_train:756.6776123\tloss_test:884.5460899\n",
      "epoch:134\tloss_train:787.1014404\tloss_test:879.1009683\n",
      "epoch:135\tloss_train:811.4056396\tloss_test:880.3223152\n",
      "epoch:136\tloss_train:736.3410645\tloss_test:876.7723169\n",
      "epoch:137\tloss_train:835.9746094\tloss_test:883.8264577\n",
      "epoch:138\tloss_train:835.9516602\tloss_test:883.3139432\n",
      "epoch:139\tloss_train:906.9786987\tloss_test:881.9758457\n",
      "epoch:140\tloss_train:788.5462646\tloss_test:874.5882257\n",
      "epoch:141\tloss_train:834.7492676\tloss_test:874.3229412\n",
      "epoch:142\tloss_train:852.0827637\tloss_test:875.6289917\n",
      "epoch:143\tloss_train:815.2839355\tloss_test:871.0456939\n",
      "epoch:144\tloss_train:855.0144653\tloss_test:878.6055493\n",
      "epoch:145\tloss_train:773.4405518\tloss_test:869.8755649\n",
      "epoch:146\tloss_train:883.9719849\tloss_test:872.4889156\n",
      "epoch:147\tloss_train:761.8618774\tloss_test:869.8690991\n",
      "epoch:148\tloss_train:809.0133057\tloss_test:874.4861300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:149\tloss_train:701.4777222\tloss_test:870.8556516\n",
      "epoch:150\tloss_train:793.2331543\tloss_test:878.9563057\n",
      "epoch:151\tloss_train:766.1208496\tloss_test:867.1112868\n",
      "epoch:152\tloss_train:827.8177490\tloss_test:865.5299544\n",
      "epoch:153\tloss_train:881.1845093\tloss_test:864.8745275\n",
      "epoch:154\tloss_train:675.4345703\tloss_test:868.6087112\n",
      "epoch:155\tloss_train:740.6918945\tloss_test:863.8675621\n",
      "epoch:156\tloss_train:774.9656372\tloss_test:865.4727499\n",
      "epoch:157\tloss_train:819.3218384\tloss_test:867.3249879\n",
      "epoch:158\tloss_train:733.2232666\tloss_test:862.8026135\n",
      "epoch:159\tloss_train:661.9528809\tloss_test:858.5976666\n",
      "epoch:160\tloss_train:868.3150024\tloss_test:862.1864475\n",
      "epoch:161\tloss_train:841.5212402\tloss_test:863.7725058\n",
      "epoch:162\tloss_train:781.5083618\tloss_test:856.5975025\n",
      "epoch:163\tloss_train:825.5590820\tloss_test:860.7132881\n",
      "epoch:164\tloss_train:668.8458252\tloss_test:857.1812612\n",
      "epoch:165\tloss_train:773.5233765\tloss_test:854.6895583\n",
      "epoch:166\tloss_train:796.5145264\tloss_test:857.5172094\n",
      "epoch:167\tloss_train:742.7276611\tloss_test:854.5046892\n",
      "epoch:168\tloss_train:819.2409668\tloss_test:855.5122229\n",
      "epoch:169\tloss_train:750.4887085\tloss_test:855.6618735\n",
      "epoch:170\tloss_train:734.5025024\tloss_test:855.4779337\n",
      "epoch:171\tloss_train:689.0531616\tloss_test:854.2965881\n",
      "epoch:172\tloss_train:924.7235718\tloss_test:853.2809210\n",
      "epoch:173\tloss_train:746.2877197\tloss_test:850.7205784\n",
      "epoch:174\tloss_train:731.6363525\tloss_test:849.6398743\n",
      "epoch:175\tloss_train:728.1094971\tloss_test:852.6522211\n",
      "epoch:176\tloss_train:699.0613403\tloss_test:855.2682965\n",
      "epoch:177\tloss_train:686.9517212\tloss_test:847.7576584\n",
      "epoch:178\tloss_train:869.9309082\tloss_test:849.2430825\n",
      "epoch:179\tloss_train:727.8304443\tloss_test:850.6734991\n",
      "epoch:180\tloss_train:753.4416504\tloss_test:851.3743725\n",
      "epoch:181\tloss_train:676.4185181\tloss_test:848.5235319\n",
      "epoch:182\tloss_train:859.4847412\tloss_test:845.8173649\n",
      "epoch:183\tloss_train:689.0114136\tloss_test:848.0314628\n",
      "epoch:184\tloss_train:817.9988403\tloss_test:848.2703340\n",
      "epoch:185\tloss_train:704.1976318\tloss_test:845.0650358\n",
      "epoch:186\tloss_train:771.7244873\tloss_test:846.0132970\n",
      "epoch:187\tloss_train:848.4714355\tloss_test:844.3128862\n",
      "epoch:188\tloss_train:810.9157104\tloss_test:843.5911571\n",
      "epoch:189\tloss_train:805.0784302\tloss_test:842.5620972\n",
      "epoch:190\tloss_train:659.6716309\tloss_test:842.8312126\n",
      "epoch:191\tloss_train:731.6913452\tloss_test:840.2871569\n",
      "epoch:192\tloss_train:868.8002930\tloss_test:841.7674619\n",
      "epoch:193\tloss_train:830.7632446\tloss_test:841.0902979\n",
      "epoch:194\tloss_train:835.3599854\tloss_test:843.7556324\n",
      "epoch:195\tloss_train:778.0566406\tloss_test:841.3440008\n",
      "epoch:196\tloss_train:872.7329102\tloss_test:841.6189717\n",
      "epoch:197\tloss_train:709.6751099\tloss_test:838.7870961\n",
      "epoch:198\tloss_train:820.7624512\tloss_test:848.2672945\n",
      "epoch:199\tloss_train:666.7432861\tloss_test:839.7571359\n",
      "epoch:200\tloss_train:914.1068726\tloss_test:840.3329818\n",
      "epoch:201\tloss_train:869.6929932\tloss_test:843.0434687\n",
      "epoch:202\tloss_train:581.3917236\tloss_test:837.4258546\n",
      "epoch:203\tloss_train:781.4786377\tloss_test:836.1617257\n",
      "epoch:204\tloss_train:750.5164795\tloss_test:837.2392423\n",
      "epoch:205\tloss_train:665.5960693\tloss_test:837.3738561\n",
      "epoch:206\tloss_train:673.7003174\tloss_test:833.3592836\n",
      "epoch:207\tloss_train:613.7500000\tloss_test:835.8686141\n",
      "epoch:208\tloss_train:758.8999634\tloss_test:834.7333835\n",
      "epoch:209\tloss_train:832.6809082\tloss_test:841.1338812\n",
      "epoch:210\tloss_train:648.4788818\tloss_test:843.9337786\n",
      "epoch:211\tloss_train:758.2213135\tloss_test:837.8834577\n",
      "epoch:212\tloss_train:647.9237061\tloss_test:835.1176947\n",
      "epoch:213\tloss_train:641.0297241\tloss_test:833.2013841\n",
      "epoch:214\tloss_train:701.2221680\tloss_test:831.1283853\n",
      "epoch:215\tloss_train:715.8612061\tloss_test:834.8515284\n",
      "epoch:216\tloss_train:770.1212769\tloss_test:832.5582567\n",
      "epoch:217\tloss_train:679.1501465\tloss_test:831.2791123\n",
      "epoch:218\tloss_train:735.6610107\tloss_test:833.4427557\n",
      "epoch:219\tloss_train:722.2940674\tloss_test:828.9141083\n",
      "epoch:220\tloss_train:816.5622559\tloss_test:834.2700629\n",
      "epoch:221\tloss_train:822.0951538\tloss_test:828.9323771\n",
      "epoch:222\tloss_train:712.4883423\tloss_test:833.5203154\n",
      "epoch:223\tloss_train:652.1127930\tloss_test:827.7263840\n",
      "epoch:224\tloss_train:900.0844727\tloss_test:831.8917243\n",
      "epoch:225\tloss_train:632.9362183\tloss_test:827.2510040\n",
      "epoch:226\tloss_train:771.2689819\tloss_test:831.6029969\n",
      "epoch:227\tloss_train:812.8221436\tloss_test:827.3350380\n",
      "epoch:228\tloss_train:736.6570435\tloss_test:825.9490635\n",
      "epoch:229\tloss_train:767.8571777\tloss_test:828.4261934\n",
      "epoch:230\tloss_train:747.9841919\tloss_test:824.5223250\n",
      "epoch:231\tloss_train:706.7631836\tloss_test:824.6931929\n",
      "epoch:232\tloss_train:708.2257080\tloss_test:825.2387200\n",
      "epoch:233\tloss_train:789.7552490\tloss_test:827.7192743\n",
      "epoch:234\tloss_train:739.2056885\tloss_test:825.5527363\n",
      "epoch:235\tloss_train:858.7304688\tloss_test:825.4307725\n",
      "epoch:236\tloss_train:755.1914673\tloss_test:823.5057245\n",
      "epoch:237\tloss_train:783.1708374\tloss_test:827.0211633\n",
      "epoch:238\tloss_train:630.4639893\tloss_test:828.4144086\n",
      "epoch:239\tloss_train:789.3796387\tloss_test:823.2844427\n",
      "epoch:240\tloss_train:717.1997070\tloss_test:823.1428162\n",
      "epoch:241\tloss_train:732.4863892\tloss_test:826.0949051\n",
      "epoch:242\tloss_train:731.0614624\tloss_test:822.2448110\n",
      "epoch:243\tloss_train:748.9030151\tloss_test:824.2163600\n",
      "epoch:244\tloss_train:701.7463379\tloss_test:829.6087395\n",
      "epoch:245\tloss_train:737.2611694\tloss_test:823.5397067\n",
      "epoch:246\tloss_train:763.1674805\tloss_test:821.8893734\n",
      "epoch:247\tloss_train:579.1613770\tloss_test:817.1880242\n",
      "epoch:248\tloss_train:667.6697388\tloss_test:819.6036358\n",
      "epoch:249\tloss_train:844.9692993\tloss_test:823.5973265\n",
      "epoch:250\tloss_train:802.8519287\tloss_test:816.9669196\n",
      "epoch:251\tloss_train:738.1312256\tloss_test:819.0721136\n",
      "epoch:252\tloss_train:663.7239380\tloss_test:820.9523652\n",
      "epoch:253\tloss_train:681.9655762\tloss_test:814.7440664\n",
      "epoch:254\tloss_train:672.5062256\tloss_test:820.2547629\n",
      "epoch:255\tloss_train:752.7952271\tloss_test:815.7919217\n",
      "epoch:256\tloss_train:634.3984375\tloss_test:815.6134910\n",
      "epoch:257\tloss_train:794.2878418\tloss_test:816.2614008\n",
      "epoch:258\tloss_train:700.4691772\tloss_test:815.5119806\n",
      "epoch:259\tloss_train:789.1690674\tloss_test:817.3443981\n",
      "epoch:260\tloss_train:693.2827759\tloss_test:815.1836039\n",
      "epoch:261\tloss_train:839.1795654\tloss_test:816.4164152\n",
      "epoch:262\tloss_train:763.7745361\tloss_test:817.5602451\n",
      "epoch:263\tloss_train:678.0166626\tloss_test:813.2126845\n",
      "epoch:264\tloss_train:706.0081177\tloss_test:816.2465585\n",
      "epoch:265\tloss_train:621.7261353\tloss_test:813.9764048\n",
      "epoch:266\tloss_train:713.9830322\tloss_test:814.4302115\n",
      "epoch:267\tloss_train:589.4312134\tloss_test:812.9595426\n",
      "epoch:268\tloss_train:745.0296631\tloss_test:818.5484901\n",
      "epoch:269\tloss_train:663.1920166\tloss_test:812.2753945\n",
      "epoch:270\tloss_train:680.0698853\tloss_test:816.0665111\n",
      "epoch:271\tloss_train:719.8047485\tloss_test:813.6985628\n",
      "epoch:272\tloss_train:728.3011475\tloss_test:812.7316628\n",
      "epoch:273\tloss_train:671.3941040\tloss_test:809.1559434\n",
      "epoch:274\tloss_train:768.7393799\tloss_test:814.9813871\n",
      "epoch:275\tloss_train:670.0150757\tloss_test:811.0572391\n",
      "epoch:276\tloss_train:667.8712769\tloss_test:812.5389761\n",
      "epoch:277\tloss_train:678.7721558\tloss_test:810.5238061\n",
      "epoch:278\tloss_train:656.8807373\tloss_test:812.5953653\n",
      "epoch:279\tloss_train:741.6560059\tloss_test:815.1526593\n",
      "epoch:280\tloss_train:677.1761475\tloss_test:809.5276722\n",
      "epoch:281\tloss_train:678.8703613\tloss_test:809.2149914\n",
      "epoch:282\tloss_train:616.9244995\tloss_test:808.1517866\n",
      "epoch:283\tloss_train:655.2717896\tloss_test:807.7449680\n",
      "epoch:284\tloss_train:702.6606445\tloss_test:810.8980976\n",
      "epoch:285\tloss_train:634.1175537\tloss_test:810.0435848\n",
      "epoch:286\tloss_train:723.7079468\tloss_test:812.1240657\n",
      "epoch:287\tloss_train:746.6328125\tloss_test:808.9089153\n",
      "epoch:288\tloss_train:738.7909546\tloss_test:809.1379622\n",
      "epoch:289\tloss_train:568.9239502\tloss_test:809.3996737\n",
      "epoch:290\tloss_train:604.0637207\tloss_test:806.3324441\n",
      "epoch:291\tloss_train:687.6107788\tloss_test:810.6546167\n",
      "epoch:292\tloss_train:653.0327148\tloss_test:807.5625736\n",
      "epoch:293\tloss_train:651.7280884\tloss_test:807.6738555\n",
      "epoch:294\tloss_train:855.7026978\tloss_test:806.9428974\n",
      "epoch:295\tloss_train:609.2999268\tloss_test:810.8804535\n",
      "epoch:296\tloss_train:660.7274170\tloss_test:805.3210806\n",
      "epoch:297\tloss_train:706.5826416\tloss_test:805.3258911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:298\tloss_train:651.0891113\tloss_test:808.8147186\n",
      "epoch:299\tloss_train:745.0322876\tloss_test:807.3714649\n",
      "epoch:300\tloss_train:707.4176636\tloss_test:811.6813346\n",
      "epoch:301\tloss_train:683.4341431\tloss_test:804.5750343\n",
      "epoch:302\tloss_train:664.7726440\tloss_test:804.6431766\n",
      "epoch:303\tloss_train:724.0640869\tloss_test:804.5756947\n",
      "epoch:304\tloss_train:711.9898071\tloss_test:805.9627055\n",
      "epoch:305\tloss_train:729.6705322\tloss_test:803.1865328\n",
      "epoch:306\tloss_train:730.4527588\tloss_test:805.6608259\n",
      "epoch:307\tloss_train:796.9467773\tloss_test:801.7468322\n",
      "epoch:308\tloss_train:700.6132202\tloss_test:802.7708411\n",
      "epoch:309\tloss_train:665.0058594\tloss_test:801.4538350\n",
      "epoch:310\tloss_train:648.5581665\tloss_test:805.2754225\n",
      "epoch:311\tloss_train:616.7827148\tloss_test:803.2809330\n",
      "epoch:312\tloss_train:796.3297119\tloss_test:812.6516915\n",
      "epoch:313\tloss_train:708.6414795\tloss_test:802.3506401\n",
      "epoch:314\tloss_train:753.5400391\tloss_test:806.6609855\n",
      "epoch:315\tloss_train:644.8314819\tloss_test:801.1803428\n",
      "epoch:316\tloss_train:674.5371094\tloss_test:810.4966274\n",
      "epoch:317\tloss_train:620.7446289\tloss_test:800.1186433\n",
      "epoch:318\tloss_train:788.9854126\tloss_test:802.6446740\n",
      "epoch:319\tloss_train:693.4448242\tloss_test:802.8921525\n",
      "epoch:320\tloss_train:656.9761353\tloss_test:801.1855920\n",
      "epoch:321\tloss_train:677.4686279\tloss_test:815.3744257\n",
      "epoch:322\tloss_train:765.0806885\tloss_test:802.9888657\n",
      "epoch:323\tloss_train:728.9825439\tloss_test:805.2650221\n",
      "epoch:324\tloss_train:701.8657227\tloss_test:802.8357236\n",
      "epoch:325\tloss_train:702.2032471\tloss_test:799.8769826\n",
      "epoch:326\tloss_train:809.8237305\tloss_test:802.7998832\n",
      "epoch:327\tloss_train:751.9672852\tloss_test:809.2482379\n",
      "epoch:328\tloss_train:590.0225830\tloss_test:803.1549126\n",
      "epoch:329\tloss_train:632.3215332\tloss_test:798.5204591\n",
      "epoch:330\tloss_train:728.5583496\tloss_test:801.7321422\n",
      "epoch:331\tloss_train:714.7548218\tloss_test:801.7618568\n",
      "epoch:332\tloss_train:634.8449707\tloss_test:802.7325778\n",
      "epoch:333\tloss_train:680.1877441\tloss_test:802.5715501\n",
      "epoch:334\tloss_train:630.6475220\tloss_test:797.2499631\n",
      "epoch:335\tloss_train:828.1738281\tloss_test:799.5146714\n",
      "epoch:336\tloss_train:692.2635498\tloss_test:795.4556604\n",
      "epoch:337\tloss_train:532.3019409\tloss_test:795.6410861\n",
      "epoch:338\tloss_train:655.9493408\tloss_test:799.8611032\n",
      "epoch:339\tloss_train:701.6817627\tloss_test:796.1575669\n",
      "epoch:340\tloss_train:586.9734497\tloss_test:802.2440306\n",
      "epoch:341\tloss_train:618.0393677\tloss_test:799.3262674\n",
      "epoch:342\tloss_train:824.6634521\tloss_test:796.4286729\n",
      "epoch:343\tloss_train:677.3721924\tloss_test:796.4825105\n",
      "epoch:344\tloss_train:627.9250488\tloss_test:797.8808982\n",
      "epoch:345\tloss_train:676.7064209\tloss_test:798.1327462\n",
      "epoch:346\tloss_train:676.3071289\tloss_test:797.6589128\n",
      "epoch:347\tloss_train:618.1182861\tloss_test:793.6798945\n",
      "epoch:348\tloss_train:631.8317871\tloss_test:795.5502881\n",
      "epoch:349\tloss_train:750.9609375\tloss_test:794.5875590\n",
      "epoch:350\tloss_train:613.6722412\tloss_test:798.0138288\n",
      "epoch:351\tloss_train:723.9102783\tloss_test:795.2922633\n",
      "epoch:352\tloss_train:700.5519409\tloss_test:797.2127353\n",
      "epoch:353\tloss_train:636.5146484\tloss_test:795.0596136\n",
      "epoch:354\tloss_train:703.2261963\tloss_test:797.1593835\n",
      "epoch:355\tloss_train:858.7324219\tloss_test:796.0897892\n",
      "epoch:356\tloss_train:637.8198242\tloss_test:793.5168552\n",
      "epoch:357\tloss_train:574.1652832\tloss_test:798.4572241\n",
      "epoch:358\tloss_train:764.3108521\tloss_test:800.8313724\n",
      "epoch:359\tloss_train:781.9171143\tloss_test:795.0147971\n",
      "epoch:360\tloss_train:771.0407715\tloss_test:793.0410296\n",
      "epoch:361\tloss_train:671.7398682\tloss_test:796.9741383\n",
      "epoch:362\tloss_train:720.3701172\tloss_test:795.3013815\n",
      "epoch:363\tloss_train:650.5255737\tloss_test:792.2114414\n",
      "epoch:364\tloss_train:749.8212280\tloss_test:792.9942237\n",
      "epoch:365\tloss_train:692.7050171\tloss_test:793.6265453\n",
      "epoch:366\tloss_train:749.8551636\tloss_test:791.8074520\n",
      "epoch:367\tloss_train:774.9769897\tloss_test:790.7375348\n",
      "epoch:368\tloss_train:769.0938721\tloss_test:798.6911465\n",
      "epoch:369\tloss_train:752.8297119\tloss_test:797.5272611\n",
      "epoch:370\tloss_train:634.0019531\tloss_test:790.4653779\n",
      "epoch:371\tloss_train:593.6602783\tloss_test:801.4958887\n",
      "epoch:372\tloss_train:758.3779297\tloss_test:791.4918509\n",
      "epoch:373\tloss_train:706.4826660\tloss_test:793.8035168\n",
      "epoch:374\tloss_train:631.1591797\tloss_test:790.8283388\n",
      "epoch:375\tloss_train:697.0485840\tloss_test:793.3774105\n",
      "epoch:376\tloss_train:699.6512451\tloss_test:795.8664366\n",
      "epoch:377\tloss_train:623.2961426\tloss_test:791.1422146\n",
      "epoch:378\tloss_train:747.6177368\tloss_test:790.9464061\n",
      "epoch:379\tloss_train:730.1180420\tloss_test:796.9640198\n",
      "epoch:380\tloss_train:632.2990723\tloss_test:796.0489059\n",
      "epoch:381\tloss_train:695.1986694\tloss_test:791.4772056\n",
      "epoch:382\tloss_train:677.7970581\tloss_test:791.1776690\n",
      "epoch:383\tloss_train:756.9045410\tloss_test:789.8188805\n",
      "epoch:384\tloss_train:575.2943115\tloss_test:796.8341128\n",
      "epoch:385\tloss_train:664.7453613\tloss_test:787.7186961\n",
      "epoch:386\tloss_train:719.9980469\tloss_test:789.9152261\n",
      "epoch:387\tloss_train:728.4397583\tloss_test:787.5533459\n",
      "epoch:388\tloss_train:638.8129883\tloss_test:795.4683881\n",
      "epoch:389\tloss_train:693.3372803\tloss_test:789.7919233\n",
      "epoch:390\tloss_train:698.9528198\tloss_test:790.2856255\n",
      "epoch:391\tloss_train:688.4911499\tloss_test:790.9480465\n",
      "epoch:392\tloss_train:687.3847046\tloss_test:795.3660711\n",
      "epoch:393\tloss_train:688.0965576\tloss_test:790.3649348\n",
      "epoch:394\tloss_train:730.0054321\tloss_test:790.4988098\n",
      "epoch:395\tloss_train:611.7694092\tloss_test:788.2038500\n",
      "epoch:396\tloss_train:785.8882446\tloss_test:786.5622092\n",
      "epoch:397\tloss_train:777.9532471\tloss_test:789.3027163\n",
      "epoch:398\tloss_train:557.0015869\tloss_test:789.2842275\n",
      "epoch:399\tloss_train:659.3766479\tloss_test:786.8935602\n",
      "epoch:400\tloss_train:597.9807129\tloss_test:794.0314902\n",
      "epoch:401\tloss_train:677.9058228\tloss_test:787.1507341\n",
      "epoch:402\tloss_train:614.7916260\tloss_test:790.3788739\n",
      "epoch:403\tloss_train:658.9531250\tloss_test:789.9891801\n",
      "epoch:404\tloss_train:636.5146484\tloss_test:789.1453921\n",
      "epoch:405\tloss_train:746.3966675\tloss_test:790.1199143\n",
      "epoch:406\tloss_train:714.7625732\tloss_test:786.6647662\n",
      "epoch:407\tloss_train:680.9159546\tloss_test:785.9828104\n",
      "epoch:408\tloss_train:640.6893311\tloss_test:786.1949122\n",
      "epoch:409\tloss_train:654.0382080\tloss_test:786.1085197\n",
      "epoch:410\tloss_train:714.7271118\tloss_test:792.7486386\n",
      "epoch:411\tloss_train:710.9000854\tloss_test:785.0760886\n",
      "epoch:412\tloss_train:701.5230713\tloss_test:786.7672286\n",
      "epoch:413\tloss_train:670.7407227\tloss_test:791.7567235\n",
      "epoch:414\tloss_train:638.1063232\tloss_test:787.5023216\n",
      "epoch:415\tloss_train:660.4449463\tloss_test:784.4378402\n",
      "epoch:416\tloss_train:676.4546509\tloss_test:788.1538458\n",
      "epoch:417\tloss_train:607.1268311\tloss_test:790.1503479\n",
      "epoch:418\tloss_train:685.2868652\tloss_test:793.5078259\n",
      "epoch:419\tloss_train:600.9962158\tloss_test:786.4382714\n",
      "epoch:420\tloss_train:642.7507935\tloss_test:787.4969244\n",
      "epoch:421\tloss_train:632.8125000\tloss_test:784.7432029\n",
      "epoch:422\tloss_train:637.4298096\tloss_test:784.8319913\n",
      "epoch:423\tloss_train:626.2984619\tloss_test:783.8555092\n",
      "epoch:424\tloss_train:670.7265625\tloss_test:785.7432633\n",
      "epoch:425\tloss_train:638.2312012\tloss_test:784.8816194\n",
      "epoch:426\tloss_train:664.5796509\tloss_test:788.3371841\n",
      "epoch:427\tloss_train:699.1188965\tloss_test:784.5793551\n",
      "epoch:428\tloss_train:594.0927124\tloss_test:786.2623599\n",
      "epoch:429\tloss_train:592.6721191\tloss_test:784.4688060\n",
      "epoch:430\tloss_train:599.9471436\tloss_test:782.6728418\n",
      "epoch:431\tloss_train:593.6301880\tloss_test:783.6926780\n",
      "epoch:432\tloss_train:663.7449951\tloss_test:783.8147119\n",
      "epoch:433\tloss_train:622.9592896\tloss_test:788.7196451\n",
      "epoch:434\tloss_train:600.5957642\tloss_test:785.8556608\n",
      "epoch:435\tloss_train:700.2152100\tloss_test:783.0969530\n",
      "epoch:436\tloss_train:576.2109375\tloss_test:782.7934923\n",
      "epoch:437\tloss_train:692.2046509\tloss_test:785.3839862\n",
      "epoch:438\tloss_train:604.0849609\tloss_test:789.3283445\n",
      "epoch:439\tloss_train:647.8645630\tloss_test:783.6507990\n",
      "epoch:440\tloss_train:672.4011230\tloss_test:782.9854658\n",
      "epoch:441\tloss_train:617.5208740\tloss_test:783.4326723\n",
      "epoch:442\tloss_train:622.4435425\tloss_test:786.7323617\n",
      "epoch:443\tloss_train:673.9396973\tloss_test:781.7407905\n",
      "epoch:444\tloss_train:690.0380859\tloss_test:786.4732410\n",
      "epoch:445\tloss_train:683.5876465\tloss_test:784.0217212\n",
      "epoch:446\tloss_train:582.7070312\tloss_test:785.4588017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:447\tloss_train:563.1577759\tloss_test:784.9112033\n",
      "epoch:448\tloss_train:620.8226318\tloss_test:786.4559504\n",
      "epoch:449\tloss_train:649.9707642\tloss_test:784.6791923\n",
      "epoch:450\tloss_train:765.6481934\tloss_test:784.3764525\n",
      "epoch:451\tloss_train:593.4718018\tloss_test:780.9390576\n",
      "epoch:452\tloss_train:759.4017334\tloss_test:787.5501464\n",
      "epoch:453\tloss_train:589.9196777\tloss_test:780.2774796\n",
      "epoch:454\tloss_train:760.7128296\tloss_test:781.3045664\n",
      "epoch:455\tloss_train:803.3821411\tloss_test:789.3434040\n",
      "epoch:456\tloss_train:587.8258057\tloss_test:779.7917665\n",
      "epoch:457\tloss_train:560.0026855\tloss_test:787.0312308\n",
      "epoch:458\tloss_train:635.1875610\tloss_test:780.8481863\n",
      "epoch:459\tloss_train:658.9741821\tloss_test:780.8592845\n",
      "epoch:460\tloss_train:638.4605713\tloss_test:782.7229016\n",
      "epoch:461\tloss_train:634.9147339\tloss_test:781.0644456\n",
      "epoch:462\tloss_train:685.2268677\tloss_test:779.5339850\n",
      "epoch:463\tloss_train:755.0346680\tloss_test:779.3602121\n",
      "epoch:464\tloss_train:788.3948364\tloss_test:782.9997714\n",
      "epoch:465\tloss_train:653.2132568\tloss_test:786.8326845\n",
      "epoch:466\tloss_train:703.7648315\tloss_test:783.0564090\n",
      "epoch:467\tloss_train:593.2510986\tloss_test:789.4860991\n",
      "epoch:468\tloss_train:633.8331909\tloss_test:786.6997624\n",
      "epoch:469\tloss_train:590.1289062\tloss_test:782.8577366\n",
      "epoch:470\tloss_train:657.2722168\tloss_test:780.3195596\n",
      "epoch:471\tloss_train:669.2272339\tloss_test:778.2262030\n",
      "epoch:472\tloss_train:574.2131958\tloss_test:782.5871845\n",
      "epoch:473\tloss_train:613.9444580\tloss_test:781.1793641\n",
      "epoch:474\tloss_train:558.7895508\tloss_test:781.4593450\n",
      "epoch:475\tloss_train:631.4561157\tloss_test:781.7242837\n",
      "epoch:476\tloss_train:602.9644775\tloss_test:782.9424760\n",
      "epoch:477\tloss_train:691.8999023\tloss_test:781.4080646\n",
      "epoch:478\tloss_train:542.9413452\tloss_test:778.9160381\n",
      "epoch:479\tloss_train:810.8045654\tloss_test:782.7253162\n",
      "epoch:480\tloss_train:556.1955566\tloss_test:780.7506359\n",
      "epoch:481\tloss_train:677.8461304\tloss_test:781.8311599\n",
      "epoch:482\tloss_train:621.9851074\tloss_test:781.0274905\n",
      "epoch:483\tloss_train:811.0294800\tloss_test:780.5453912\n",
      "epoch:484\tloss_train:616.5560303\tloss_test:782.2078103\n",
      "epoch:485\tloss_train:610.8670044\tloss_test:778.2260559\n",
      "epoch:486\tloss_train:668.5097046\tloss_test:782.5320380\n",
      "epoch:487\tloss_train:680.3164062\tloss_test:784.2207506\n",
      "epoch:488\tloss_train:655.0387573\tloss_test:779.4179962\n",
      "epoch:489\tloss_train:638.9512329\tloss_test:786.3424051\n",
      "epoch:490\tloss_train:690.0387573\tloss_test:779.2933290\n",
      "epoch:491\tloss_train:661.6683350\tloss_test:776.6373307\n",
      "epoch:492\tloss_train:691.2832031\tloss_test:780.9116727\n",
      "epoch:493\tloss_train:675.2888794\tloss_test:778.1644375\n",
      "epoch:494\tloss_train:614.2006836\tloss_test:778.2349899\n",
      "epoch:495\tloss_train:624.9109497\tloss_test:778.4541186\n",
      "epoch:496\tloss_train:799.9476929\tloss_test:777.9960055\n",
      "epoch:497\tloss_train:636.0368652\tloss_test:778.8582710\n",
      "epoch:498\tloss_train:625.4842529\tloss_test:779.5304471\n",
      "epoch:499\tloss_train:660.5144043\tloss_test:778.2550258\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "batch_size = 40\n",
    "n_batches = image_train.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(n_epochs):    \n",
    "    idx = np.random.permutation(image_train.shape[0])\n",
    "    X_batches = np.array_split(image_train[idx], n_batches)\n",
    "\n",
    "    for X_batch, y_batch in zip(X_batches, X_batches):\n",
    "        loss_train = autoencoder_net.train_on_batch(X_batch,y_batch)\n",
    "\n",
    "    loss_test = autoencoder_net.evaluate(x=image_test, y=image_test, batch_size=batch_size, verbose=0)\n",
    "    \n",
    "    print \"epoch:{}\\tloss_train:{:.7f}\\tloss_test:{:.7f}\".format(epoch, loss_train, loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder_net.save('./data/whale/autoencoder_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "# del autoencoder_net  # deletes the existing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "autoencoder_net = load_model('./data/whale/autoencoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

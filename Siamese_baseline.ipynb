{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from math import floor, ceil, pi\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load raw images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/whale/train_full/00022e1a.jpg', './data/whale/train_full/000466c4.jpg', './data/whale/train_full/00087b01.jpg', './data/whale/train_full/001296d5.jpg', './data/whale/train_full/0014cfdf.jpg', './data/whale/train_full/0025e8c2.jpg', './data/whale/train_full/0026a8ab.jpg', './data/whale/train_full/0031c258.jpg', './data/whale/train_full/0035632e.jpg', './data/whale/train_full/0037e7d3.jpg', './data/whale/train_full/00389cd7.jpg', './data/whale/train_full/0042dcc4.jpg', './data/whale/train_full/0042ea34.jpg', './data/whale/train_full/00467ae9.jpg', './data/whale/train_full/004a97f3.jpg', './data/whale/train_full/004c5fb9.jpg', './data/whale/train_full/005c57e7.jpg', './data/whale/train_full/006d0aaf.jpg', './data/whale/train_full/0078af23.jpg', './data/whale/train_full/007c3603.jpg']\n"
     ]
    }
   ],
   "source": [
    "def get_image_paths():\n",
    "    folder = './data/whale/train_full'\n",
    "    files = os.listdir(folder)\n",
    "    files.sort()\n",
    "    files = ['{}/{}'.format(folder, file) for file in files]\n",
    "    return files\n",
    "\n",
    "X_img_paths = get_image_paths()\n",
    "print(X_img_paths[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load whale ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00022e1a.jpg</td>\n",
       "      <td>w_e15442c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000466c4.jpg</td>\n",
       "      <td>w_1287fbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00087b01.jpg</td>\n",
       "      <td>w_da2efe0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001296d5.jpg</td>\n",
       "      <td>w_19e5482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0014cfdf.jpg</td>\n",
       "      <td>w_f22f3e3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image         Id\n",
       "0  00022e1a.jpg  w_e15442c\n",
       "1  000466c4.jpg  w_1287fbc\n",
       "2  00087b01.jpg  w_da2efe0\n",
       "3  001296d5.jpg  w_19e5482\n",
       "4  0014cfdf.jpg  w_f22f3e3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_labels():\n",
    "    csv_file = './data/whale/train.csv'\n",
    "    data_labels = pd.read_csv(csv_file)\n",
    "    return data_labels\n",
    "\n",
    "data_labels = load_labels()\n",
    "data_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Id analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new_whale    810\n",
       "w_1287fbc     34\n",
       "w_98baff9     27\n",
       "w_7554f44     26\n",
       "w_1eafe46     23\n",
       "w_693c9ee     22\n",
       "w_fd1cb9d     22\n",
       "w_ab4cae2     22\n",
       "w_43be268     21\n",
       "w_73d5489     21\n",
       "w_987a36f     21\n",
       "w_f19faeb     20\n",
       "w_9b401eb     19\n",
       "w_95874a5     19\n",
       "w_c0d494d     18\n",
       "w_b7d5069     18\n",
       "w_dbda0d6     17\n",
       "w_0e737d0     17\n",
       "w_eb0a6ed     17\n",
       "w_18eee6e     17\n",
       "w_17ee910     16\n",
       "w_b0e05b1     16\n",
       "w_6c803bf     16\n",
       "w_67de30b     16\n",
       "w_a59905f     16\n",
       "w_9ca943b     15\n",
       "w_89e159a     15\n",
       "w_ee17a08     15\n",
       "w_cae7677     15\n",
       "w_b074cdf     14\n",
       "            ... \n",
       "w_ed67618      1\n",
       "w_ddb4c8d      1\n",
       "w_e3b3ade      1\n",
       "w_771136b      1\n",
       "w_4e85c68      1\n",
       "w_9dcf002      1\n",
       "w_874cf52      1\n",
       "w_9845f16      1\n",
       "w_d7ffaf2      1\n",
       "w_ebb16ab      1\n",
       "w_dfd3f5e      1\n",
       "w_985877e      1\n",
       "w_8e93d0e      1\n",
       "w_8bcf29b      1\n",
       "w_a69bb2b      1\n",
       "w_471ae98      1\n",
       "w_f4f3f6d      1\n",
       "w_959b917      1\n",
       "w_0b0d88d      1\n",
       "w_d4251cb      1\n",
       "w_f2d87b0      1\n",
       "w_a91600a      1\n",
       "w_ee23a5f      1\n",
       "w_45aac7a      1\n",
       "w_b856fc1      1\n",
       "w_4e505cc      1\n",
       "w_9874f0d      1\n",
       "w_5102893      1\n",
       "w_224000c      1\n",
       "w_f78c287      1\n",
       "Name: Id, Length: 4251, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_labels['Id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select ids occur more than 4 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_label_subset_by_threshold(occurence=5):\n",
    "    value_statics = data_labels['Id'].value_counts()\n",
    "    value_cut = value_statics[value_statics >= occurence]\n",
    "    value_cut = value_cut[value_cut < 100] #remove the \"new whale\" type\n",
    "    return value_cut  #return type: pandas series\n",
    "\n",
    "label_subset = get_label_subset_by_threshold(occurence=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation: to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_augmentation(original_image):\n",
    "    #to do\n",
    "    \n",
    "    return original_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_include_subset(label, label_subset):\n",
    "    if label in label_subset.index:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# image resize and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2508, 224, 224, 1)\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 224\n",
    "\n",
    "def tf_resize_augment_images(X_img_file_paths):\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, (None, None, 1))\n",
    "    tf_img = tf.image.resize_images(X, (IMAGE_SIZE, IMAGE_SIZE), tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Each image is resized individually as different image may be of different size.\n",
    "        for index, file_path in enumerate(X_img_file_paths):\n",
    "            label = data_labels.iloc[index]['Id']\n",
    "            \n",
    "            if is_include_subset(label, label_subset):\n",
    "                img = mpimg.imread(file_path)\n",
    "                if len(img.shape) > 2:# convert to grayscale\n",
    "                    img = rgb2gray(img)\n",
    "                img = img.reshape(img.shape[0], img.shape[1], 1)\n",
    "                resized_img = sess.run(tf_img, feed_dict = {X: img})\n",
    "                X_data.append(data_augmentation(resized_img))\n",
    "                y_data.append(label)\n",
    "\n",
    "    X_data = np.array(X_data, dtype = np.float32) # Convert to numpy\n",
    "    return X_data, y_data\n",
    "\n",
    "X_imgs, y_data = tf_resize_augment_images(X_img_paths)\n",
    "print(X_imgs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save processed images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./data/whale/save/resize.npy', X_imgs)\n",
    "labels_dataframe = pd.DataFrame({'labels':y_data})\n",
    "labels_dataframe.to_csv('./data/whale/save/labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>w_1287fbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>w_da2efe0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>w_3d0bc7a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w_fd1cb9d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>w_ab6db0f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      labels\n",
       "0  w_1287fbc\n",
       "1  w_da2efe0\n",
       "2  w_3d0bc7a\n",
       "3  w_fd1cb9d\n",
       "4  w_ab6db0f"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_imgs = np.load('./data/whale/save/resize.npy')\n",
    "labels_dataframe = pd.read_csv('./data/whale/save/labels.csv')\n",
    "labels_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate image pairs and labels(true or false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_image_pair_and_labels(images, labels, data_size=500):\n",
    "    size1 = data_size // 2 \n",
    "    size2 = data_size - size1\n",
    "    if size1 < size2 and np.random.rand() > 0.5:\n",
    "        size1, size2 = size2, size1\n",
    "    \n",
    "    X, y = [], []\n",
    "    k = 0\n",
    "    while k < size1:\n",
    "        idx1, idx2 = np.random.randint(0, len(labels), 2)\n",
    "        if idx1 != idx2 and labels[idx1] == labels[idx2]:\n",
    "            X.append(np.array([images[idx1], images[idx2]]))\n",
    "            y.append([1])\n",
    "            k += 1\n",
    "    k = 0\n",
    "    while k < size2:\n",
    "        idx1, idx2 = np.random.randint(0, len(labels), 2)\n",
    "        if labels[idx1] != labels[idx2]:\n",
    "            X.append(np.array([images[idx1], images[idx2]]))\n",
    "            y.append([0])\n",
    "            k += 1\n",
    "            \n",
    "    shuffled_idx = np.random.permutation(data_size)\n",
    "    return np.array(X)[shuffled_idx], np.array(y)[shuffled_idx]\n",
    "\n",
    "#select 100 pairs to tain siamese network, just for quick test\n",
    "image_pairs, y_labels = generate_image_pair_and_labels(X_imgs, labels_dataframe['labels'].tolist(), data_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ratio = 0.3\n",
    "image_pair_train = image_pairs[:int((1 - test_ratio) * len(y_labels))]\n",
    "y_labels_train = y_labels[:int((1 - test_ratio) * len(y_labels))]\n",
    "image_pair_test = image_pairs[int((1 - test_ratio) * len(y_labels)):]\n",
    "y_labels_test = y_labels[int((1 - test_ratio) * len(y_labels)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rerun the model from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define residual network block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resNet_block(inputs, channel, strides, activation=tf.nn.relu, reuse=False, name=None):\n",
    "    net = tf.layers.conv2d(inputs, channel, 3, strides=strides, kernel_initializer=he_init, activation=activation, \n",
    "                           padding=\"same\", reuse=reuse, name=name+'_conv1')\n",
    "    net = tf.layers.conv2d(net, channel, 3, strides=1, kernel_initializer=he_init, padding=\"same\", reuse=reuse, name=name+'_conv2')\n",
    "\n",
    "    if strides > 1:\n",
    "        inputs = tf.layers.conv2d(inputs, channel, 3, strides=strides, kernel_initializer=he_init, padding=\"same\", \n",
    "                                  reuse=reuse, name=name+'_conv3')\n",
    "    net = tf.add_n([inputs, net])\n",
    "    net = activation(net)\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define residual network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_network(inputs, activation=tf.nn.relu, reuse=False, name=None):\n",
    "    net = tf.layers.conv2d(inputs, 32, 7, strides=2, kernel_initializer=he_init, activation=activation, \n",
    "                           padding=\"same\", reuse=reuse, name=name+'_conv1')\n",
    "    net = tf.layers.max_pooling2d(net, 2, 2)\n",
    "    \n",
    "    net = resNet_block(net, 32, 1, activation=activation, reuse=reuse, name=name+'_block1')\n",
    "    net = resNet_block(net, 32, 1, activation=activation, reuse=reuse, name=name+'_block2')\n",
    "    net = resNet_block(net, 32, 1, activation=activation, reuse=reuse, name=name+'_block3')\n",
    "    net = resNet_block(net, 64, 2, activation=activation, reuse=reuse, name=name+'_block4')\n",
    "    net = resNet_block(net, 64, 1, activation=activation, reuse=reuse, name=name+'_block5')\n",
    "    net = resNet_block(net, 64, 1, activation=activation, reuse=reuse, name=name+'_block6')\n",
    "    net = resNet_block(net, 128, 2, activation=activation, reuse=reuse, name=name+'_block7')\n",
    "    net = resNet_block(net, 128, 1, activation=activation, reuse=reuse, name=name+'_block8')\n",
    "    net = resNet_block(net, 128, 1, activation=activation, reuse=reuse, name=name+'_block9')\n",
    "    \n",
    "    net = tf.contrib.layers.flatten(net)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define siamese netwrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def siamese_network(image1, image2):\n",
    "    embedding1 = residual_network(image1, activation=tf.nn.relu, reuse=False, name='resNet1')\n",
    "    embedding2 = residual_network(image2, activation=tf.nn.relu, reuse=True, name='resNet1')\n",
    "    net = tf.concat([embedding1, embedding2], axis=1)\n",
    "    net = tf.layers.dense(net, units=20, activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "    logits = tf.layers.dense(net, units=1, kernel_initializer=he_init, name=\"outputs\")\n",
    "    y_prob = tf.nn.sigmoid(logits)\n",
    "    \n",
    "    return logits, y_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define train module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(logits, y_prob, y_labels):\n",
    "    loss = tf.losses.log_loss(y_labels, y_prob)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.00001)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    y_pred = tf.cast(tf.greater_equal(logits, 0), tf.int32)\n",
    "    y_pred_correct = tf.equal(y_pred, y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(y_pred_correct, tf.float32))\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "    accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    return loss, train_op, accuracy, summary_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# starting training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height = 224\n",
    "width = 224\n",
    "channel = 1\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 2, height, width, channel), name=\"X\")\n",
    "X1, X2 = tf.unstack(X, axis=1)\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "logits, y_prob = siamese_network(X1, X2)\n",
    "loss, train_op, accuracy, summary_op = train(logits, y_prob, y)\n",
    "init = tf.global_variables_initializer()\n",
    "# print([x.name for x in tf.global_variables()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tloss_train:6.447\tloss_test:6.985\tacc_train:60.00%\tacc_test:56.67%\n",
      "1\tloss_train:9.671\tloss_test:6.985\tacc_train:40.00%\tacc_test:56.67%\n",
      "2\tloss_train:12.894\tloss_test:6.985\tacc_train:20.00%\tacc_test:56.67%\n",
      "3\tloss_train:9.671\tloss_test:6.985\tacc_train:40.00%\tacc_test:56.67%\n",
      "4\tloss_train:9.671\tloss_test:6.985\tacc_train:40.00%\tacc_test:56.67%\n",
      "5\tloss_train:16.118\tloss_test:6.985\tacc_train:0.00%\tacc_test:56.67%\n",
      "6\tloss_train:6.447\tloss_test:6.985\tacc_train:60.00%\tacc_test:56.67%\n",
      "7\tloss_train:12.894\tloss_test:6.985\tacc_train:20.00%\tacc_test:56.67%\n",
      "8\tloss_train:9.671\tloss_test:6.985\tacc_train:40.00%\tacc_test:56.67%\n",
      "9\tloss_train:3.224\tloss_test:6.985\tacc_train:80.00%\tacc_test:56.67%\n",
      "10\tloss_train:9.671\tloss_test:6.985\tacc_train:40.00%\tacc_test:56.67%\n",
      "11\tloss_train:9.671\tloss_test:6.985\tacc_train:40.00%\tacc_test:56.67%\n",
      "12\tloss_train:6.447\tloss_test:6.985\tacc_train:60.00%\tacc_test:56.67%\n",
      "13\tloss_train:6.447\tloss_test:6.985\tacc_train:60.00%\tacc_test:56.67%\n",
      "14\tloss_train:12.894\tloss_test:6.985\tacc_train:20.00%\tacc_test:56.67%\n",
      "15\tloss_train:3.224\tloss_test:6.985\tacc_train:80.00%\tacc_test:56.67%\n",
      "16\tloss_train:6.447\tloss_test:6.985\tacc_train:60.00%\tacc_test:56.67%\n",
      "17\tloss_train:9.671\tloss_test:6.985\tacc_train:40.00%\tacc_test:56.67%\n",
      "18\tloss_train:6.447\tloss_test:6.985\tacc_train:60.00%\tacc_test:56.67%\n",
      "19\tloss_train:6.447\tloss_test:6.985\tacc_train:60.00%\tacc_test:56.67%\n",
      "20\tloss_train:9.671\tloss_test:6.985\tacc_train:40.00%\tacc_test:56.67%\n",
      "21\tloss_train:9.671\tloss_test:6.985\tacc_train:40.00%\tacc_test:56.67%\n",
      "22\tloss_train:6.447\tloss_test:6.985\tacc_train:60.00%\tacc_test:56.67%\n",
      "23\tloss_train:3.224\tloss_test:6.985\tacc_train:80.00%\tacc_test:56.67%\n",
      "24\tloss_train:12.894\tloss_test:6.985\tacc_train:20.00%\tacc_test:56.67%\n",
      "25\tloss_train:6.447\tloss_test:6.985\tacc_train:60.00%\tacc_test:56.67%\n",
      "26\tloss_train:9.671\tloss_test:6.985\tacc_train:40.00%\tacc_test:56.67%\n",
      "27\tloss_train:9.671\tloss_test:6.985\tacc_train:40.00%\tacc_test:56.67%\n",
      "28\tloss_train:9.671\tloss_test:6.985\tacc_train:40.00%\tacc_test:56.67%\n",
      "29\tloss_train:6.447\tloss_test:6.985\tacc_train:60.00%\tacc_test:56.67%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "batch_size = 5\n",
    "\n",
    "n_batches = len(y_labels_train) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "        \n",
    "    for epoch in range(n_epochs):    \n",
    "        idx = np.random.permutation(len(y_labels_train))\n",
    "        X_batches = np.array_split(image_pair_train[idx], n_batches)\n",
    "        y_batches = np.array_split(y_labels_train[idx], n_batches)\n",
    "#         print X_batches[0].shape\n",
    "        \n",
    "        for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "            sess.run(train_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_train, accuracy_train = sess.run([loss, accuracy], feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_test, accuracy_test = sess.run([loss, accuracy], feed_dict={X: image_pair_test, y: y_labels_test})\n",
    "        \n",
    "        print \"{}\\tloss_train:{:.3f}\\tloss_test:{:.3f}\\tacc_train:{:.2f}%\\tacc_test:{:.2f}%\".format(\n",
    "            epoch, loss_train, loss_test, accuracy_train*100, accuracy_test*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
